import os
from pathlib import Path
from typing import Optional, List
from pydantic import Field
from pydantic_settings import BaseSettings
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
env_path = Path(__file__).parent.parent / ".env"
load_dotenv(dotenv_path=env_path)

class Settings(BaseSettings):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • í´ë˜ìŠ¤ (MLOps í™•ì¥)"""
    
    # === API ì„¤ì • ===
    openai_api_key: str = Field(..., env="OPENAI_API_KEY")
    openai_model: str = Field(default="gpt-4o-mini", env="OPENAI_MODEL")
    openai_temperature: float = Field(default=0.7, env="OPENAI_TEMPERATURE")
    openai_max_tokens: int = Field(default=1000, env="OPENAI_MAX_TOKENS")
    
    # === ì„œë²„ ì„¤ì • ===
    host: str = Field(default="0.0.0.0", env="HOST")
    port: int = Field(default=8000, env="PORT")
    debug: bool = Field(default=True, env="DEBUG")
    reload: bool = Field(default=True, env="RELOAD")
    
    # === ëª¨ë¸ ì„¤ì • ===
    kanana_model_name: str = Field(
        default="kakaocorp/kanana-1.5-2.1b-instruct-2505", 
        env="KANANA_MODEL_NAME"
    )
    kanana_finetuned_path: Optional[str] = Field(
        default="./kanana-vector-restoration", 
        env="KANANA_FINETUNED_PATH"
    )
    device: str = Field(default="auto", env="DEVICE")  # auto, cuda, cpu
    dtype: str = Field(default="bfloat16", env="MODEL_DTYPE")
    
    # === ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ===
    chroma_data_path: str = Field(default="./data/chroma_data", env="CHROMA_DATA_PATH")
    chroma_collection_name: str = Field(
        default="kanana-docs-optimized", 
        env="CHROMA_COLLECTION_NAME"
    )
    
    # === ë©”ëª¨ë¦¬ ì„¤ì • ===
    memory_max_count: int = Field(default=30, env="MEMORY_MAX_COUNT")
    memory_auto_save: bool = Field(default=True, env="MEMORY_AUTO_SAVE")
    memory_save_path: str = Field(default="./data/memory", env="MEMORY_SAVE_PATH")
    
    # === ê²€ìƒ‰ ì„¤ì • ===
    search_default_results: int = Field(default=3, env="SEARCH_DEFAULT_RESULTS")
    search_max_results: int = Field(default=10, env="SEARCH_MAX_RESULTS")
    
    # === ë¡œê¹… ì„¤ì • ===
    log_level: str = Field(default="INFO", env="LOG_LEVEL")
    log_file_path: str = Field(default="./logs/app.log", env="LOG_FILE_PATH")
    log_file_rotation: str = Field(default="10 MB", env="LOG_FILE_ROTATION")
    
    # === CORS ì„¤ì • ===
    allowed_origins: List[str] = Field(
        default=["http://localhost:3000", "http://127.0.0.1:3000"],
        env="CORS_ORIGINS"
    )
    
    # === ì„±ëŠ¥ ì„¤ì • ===
    max_concurrent_requests: int = Field(default=10, env="MAX_CONCURRENT_REQUESTS")
    request_timeout: int = Field(default=300, env="REQUEST_TIMEOUT")
    
    # ================================
    # ğŸš€ ìƒˆë¡œìš´ MLOps íŒŒì¸íŠœë‹ ì„¤ì •
    # ================================
    
    # === íŒŒì¸íŠœë‹ ìë™í™” ì„¤ì • ===
    finetune_enabled: bool = Field(default=True, env="FINETUNE_ENABLED")
    finetune_batch_size: int = Field(default=50, env="FINETUNE_BATCH_SIZE")  # ëª‡ ê°œ ëŒ€í™” ìŒ“ì´ë©´ í•™ìŠµ
    finetune_auto_trigger: bool = Field(default=True, env="FINETUNE_AUTO_TRIGGER")  # ìë™ íŠ¸ë¦¬ê±° ì—¬ë¶€
    
    # === íŒŒì¸íŠœë‹ ë°ì´í„° ê²½ë¡œ ===
    finetune_data_path: str = Field(default="./data/finetune", env="FINETUNE_DATA_PATH")
    finetune_conversations_file: str = Field(
        default="conversations.jsonl", 
        env="FINETUNE_CONVERSATIONS_FILE"
    )  # ëŒ€í™” ë°ì´í„° íŒŒì¼ëª…
    finetune_dataset_file: str = Field(
        default="training_dataset.json", 
        env="FINETUNE_DATASET_FILE"
    )  # ìƒì„±ëœ í•™ìŠµ ë°ì´í„°ì…‹
    
    # === íŒŒì¸íŠœë‹ ëª¨ë¸ ê´€ë¦¬ ===
    finetune_models_path: str = Field(default="./models", env="FINETUNE_MODELS_PATH")
    finetune_backup_count: int = Field(default=3, env="FINETUNE_BACKUP_COUNT")  # ë°±ì—… ë³´ê´€ ê°œìˆ˜
    finetune_version_prefix: str = Field(default="v", env="FINETUNE_VERSION_PREFIX")  # ë²„ì „ ì ‘ë‘ì‚¬
    
    # === íŒŒì¸íŠœë‹ í•˜ì´í¼íŒŒë¼ë¯¸í„° ===
    finetune_epochs: int = Field(default=2, env="FINETUNE_EPOCHS")
    finetune_learning_rate: float = Field(default=1e-4, env="FINETUNE_LEARNING_RATE")
    finetune_lora_r: int = Field(default=16, env="FINETUNE_LORA_R")
    finetune_lora_alpha: int = Field(default=32, env="FINETUNE_LORA_ALPHA")
    finetune_lora_dropout: float = Field(default=0.1, env="FINETUNE_LORA_DROPOUT")
    
    # === ëŒ€í™” ìˆ˜ì§‘ ì„¤ì • ===
    conversation_collection_enabled: bool = Field(
        default=True, 
        env="CONVERSATION_COLLECTION_ENABLED"
    )
    conversation_min_length: int = Field(default=5, env="CONVERSATION_MIN_LENGTH")  # ìµœì†Œ ë¬¸ì ìˆ˜
    conversation_max_length: int = Field(default=2000, env="CONVERSATION_MAX_LENGTH")  # ìµœëŒ€ ë¬¸ì ìˆ˜
    conversation_filter_system: bool = Field(
        default=True, 
        env="CONVERSATION_FILTER_SYSTEM"
    )  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ í•„í„°ë§
    
    # === ëª¨ë‹ˆí„°ë§ ì„¤ì • ===
    finetune_monitoring_enabled: bool = Field(default=True, env="FINETUNE_MONITORING_ENABLED")
    finetune_webhook_url: Optional[str] = Field(default=None, env="FINETUNE_WEBHOOK_URL")  # ìŠ¬ë™/ë””ìŠ¤ì½”ë“œ ì›¹í›…
    
    model_config = {
        "env_file": ".env",
        "env_file_encoding": "utf-8",
        "case_sensitive": False,
        "protected_namespaces": ('settings_',),
        "env_nested_delimiter": "__",
        "extra": "ignore"
    }
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._validate_settings()
        self._create_directories()
    
    def _validate_settings(self):
        """ì„¤ì • ê°’ ìœ íš¨ì„± ê²€ì‚¬"""
        if not self.openai_api_key:
            print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if self.device not in ["auto", "cuda", "cpu"]:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë””ë°”ì´ìŠ¤ ì„¤ì •: {self.device}. 'auto'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
            self.device = "auto"
        
        if self.memory_max_count <= 0:
            raise ValueError("MEMORY_MAX_COUNTëŠ” 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
        
        # ğŸš€ MLOps ì„¤ì • ê²€ì¦
        if self.finetune_batch_size <= 0:
            raise ValueError("FINETUNE_BATCH_SIZEëŠ” 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
        
        if self.finetune_backup_count < 0:
            raise ValueError("FINETUNE_BACKUP_COUNTëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        if self.conversation_min_length >= self.conversation_max_length:
            raise ValueError("CONVERSATION_MIN_LENGTHëŠ” MAX_LENGTHë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤.")
    
    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í„°ë¦¬ ìƒì„±"""
        directories = [
            self.chroma_data_path,
            self.memory_save_path,
            self.finetune_data_path,  # ğŸš€ íŒŒì¸íŠœë‹ ë°ì´í„° ê²½ë¡œ
            self.finetune_models_path,  # ğŸš€ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            os.path.dirname(self.log_file_path) if self.log_file_path else None
        ]
        
        for directory in directories:
            if directory:
                Path(directory).mkdir(parents=True, exist_ok=True)
    
    def get_model_config(self) -> dict:
        """ëª¨ë¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {
            "model_name": self.kanana_model_name,
            "finetuned_path": self.kanana_finetuned_path,
            "device": self.device,
            "dtype": self.dtype
        }
    
    def get_chroma_config(self) -> dict:
        """ChromaDB ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {
            "path": self.chroma_data_path,
            "collection_name": self.chroma_collection_name
        }
    
    def get_openai_config(self) -> dict:
        """OpenAI ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {
            "api_key": self.openai_api_key,
            "model": self.openai_model,
            "temperature": self.openai_temperature,
            "max_tokens": self.openai_max_tokens
        }
    
    # ğŸš€ ìƒˆë¡œìš´ MLOps ì„¤ì • ë©”ì„œë“œë“¤
    def get_finetune_config(self) -> dict:
        """íŒŒì¸íŠœë‹ ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {
            "enabled": self.finetune_enabled,
            "batch_size": self.finetune_batch_size,
            "auto_trigger": self.finetune_auto_trigger,
            "data_path": self.finetune_data_path,
            "conversations_file": self.finetune_conversations_file,
            "dataset_file": self.finetune_dataset_file,
            "models_path": self.finetune_models_path,
            "backup_count": self.finetune_backup_count,
            "version_prefix": self.finetune_version_prefix,
            "hyperparameters": {
                "epochs": self.finetune_epochs,
                "learning_rate": self.finetune_learning_rate,
                "lora_r": self.finetune_lora_r,
                "lora_alpha": self.finetune_lora_alpha,
                "lora_dropout": self.finetune_lora_dropout
            }
        }
    
    def get_conversation_config(self) -> dict:
        """ëŒ€í™” ìˆ˜ì§‘ ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {
            "enabled": self.conversation_collection_enabled,
            "min_length": self.conversation_min_length,
            "max_length": self.conversation_max_length,
            "filter_system": self.conversation_filter_system,
            "data_path": self.finetune_data_path,
            "file_name": self.finetune_conversations_file
        }
    
    def get_full_conversation_path(self) -> str:
        """ëŒ€í™” ë°ì´í„° ì „ì²´ ê²½ë¡œ ë°˜í™˜"""
        return os.path.join(self.finetune_data_path, self.finetune_conversations_file)
    
    def get_full_dataset_path(self) -> str:
        """í•™ìŠµ ë°ì´í„°ì…‹ ì „ì²´ ê²½ë¡œ ë°˜í™˜"""
        return os.path.join(self.finetune_data_path, self.finetune_dataset_file)
    
    def print_settings_summary(self):
        """ì„¤ì • ìš”ì•½ ì¶œë ¥ (MLOps í¬í•¨)"""
        print("\nğŸ”§ === ì‹œìŠ¤í…œ ì„¤ì • ===")
        print(f"ğŸŒ ì„œë²„: {self.host}:{self.port}")
        print(f"ğŸ¤– ëª¨ë¸: {self.kanana_model_name}")
        print(f"ğŸ”¥ ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"ğŸ—„ï¸ ChromaDB: {self.chroma_data_path}")
        print(f"ğŸ§  ë©”ëª¨ë¦¬: {self.memory_max_count}ê°œ ë©”ì‹œì§€")
        print(f"ğŸ” ê²€ìƒ‰: ê¸°ë³¸ {self.search_default_results}ê°œ ê²°ê³¼")
        print(f"ğŸ“ ë¡œê·¸: {self.log_level} â†’ {self.log_file_path}")
        print(f"ğŸŒ CORS: {', '.join(self.allowed_origins)}")
        
        # ğŸš€ MLOps ì„¤ì • ìš”ì•½
        print("\nğŸš€ === MLOps ì„¤ì • ===")
        print(f"ğŸ¤– íŒŒì¸íŠœë‹: {'í™œì„±í™”' if self.finetune_enabled else 'ë¹„í™œì„±í™”'}")
        print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {self.finetune_batch_size}ê°œ ëŒ€í™”")
        print(f"âš¡ ìë™ íŠ¸ë¦¬ê±°: {'ON' if self.finetune_auto_trigger else 'OFF'}")
        print(f"ğŸ“ ë°ì´í„° ê²½ë¡œ: {self.finetune_data_path}")
        print(f"ğŸ’¾ ëª¨ë¸ ë°±ì—…: {self.finetune_backup_count}ê°œ ë³´ê´€")
        print(f"ğŸ“ˆ ëŒ€í™” ìˆ˜ì§‘: {'í™œì„±í™”' if self.conversation_collection_enabled else 'ë¹„í™œì„±í™”'}")
        print(f"ğŸ“ ëŒ€í™” ê¸¸ì´: {self.conversation_min_length}~{self.conversation_max_length}ì")
        print("ğŸš€ ====================\n")

# ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
settings = Settings()

# ì„¤ì • ë¡œë“œ í™•ì¸ í•¨ìˆ˜
def check_environment():
    """í™˜ê²½ ì„¤ì • ìƒíƒœ í™•ì¸ (MLOps í¬í•¨)"""
    issues = []
    
    # ê¸°ì¡´ ê²€ì¦
    if not settings.openai_api_key:
        issues.append("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        print("âœ… OpenAI API í‚¤ ë¡œë“œë¨")
    
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    if settings.kanana_finetuned_path and not Path(settings.kanana_finetuned_path).exists():
        issues.append(f"âš ï¸ íŒŒì¸íŠœë‹ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {settings.kanana_finetuned_path}")
    
    # ğŸš€ MLOps ë””ë ‰í„°ë¦¬ ê¶Œí•œ í™•ì¸
    mlops_paths = [
        settings.chroma_data_path,
        settings.memory_save_path,
        settings.finetune_data_path,
        settings.finetune_models_path
    ]
    
    for path in mlops_paths:
        try:
            Path(path).mkdir(parents=True, exist_ok=True)
            test_file = Path(path) / "test_write"
            test_file.touch()
            test_file.unlink()
        except Exception as e:
            issues.append(f"âŒ ë””ë ‰í„°ë¦¬ ì“°ê¸° ê¶Œí•œ ì—†ìŒ: {path} ({e})")
    
    # ğŸš€ íŒŒì¸íŠœë‹ ì„¤ì • ê²€ì¦
    if settings.finetune_enabled:
        print("âœ… íŒŒì¸íŠœë‹ ìë™í™” í™œì„±í™”ë¨")
        if settings.finetune_auto_trigger:
            print(f"âœ… ìë™ íŠ¸ë¦¬ê±°: {settings.finetune_batch_size}ê°œ ëŒ€í™”ë§ˆë‹¤ ì‹¤í–‰")
        else:
            print("âš ï¸ ìë™ íŠ¸ë¦¬ê±° ë¹„í™œì„±í™”ë¨ (ìˆ˜ë™ ì‹¤í–‰ í•„ìš”)")
    
    if settings.conversation_collection_enabled:
        print("âœ… ëŒ€í™” ìˆ˜ì§‘ í™œì„±í™”ë¨")
        print(f"ğŸ“ ìˆ˜ì§‘ ì¡°ê±´: {settings.conversation_min_length}~{settings.conversation_max_length}ì")
    
    if issues:
        print("\nğŸš¨ === í™˜ê²½ ì„¤ì • ë¬¸ì œ ===")
        for issue in issues:
            print(issue)
        print("ğŸš¨ ========================\n")
        return False
    else:
        print("âœ… ëª¨ë“  í™˜ê²½ ì„¤ì •ì´ ì˜¬ë°”ë¦…ë‹ˆë‹¤!")
        return True