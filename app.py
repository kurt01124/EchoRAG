from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import uvicorn
import asyncio
import time
from datetime import datetime

from config.settings import settings, check_environment
from models.kanana_model import KananaModel
from services.vector_service import VectorService
from services.gpt_service import GPTService
from utils.memory_manager import MessageQueue

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(title="RAG Chat API", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
            allow_origins=settings.allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜ë“¤
kanana_model = None
vector_service = None
gpt_service = None
memory = MessageQueue(cnt=settings.memory_max_count)
stats = {
    "total_queries": 0,
    "total_search_time": 0,
    "total_gpt_time": 0,
    "total_embedding_time": 0
}

# Pydantic ëª¨ë¸ë“¤
class ChatMessage(BaseModel):
    message: str
    user_id: Optional[str] = "default"

class ChatResponse(BaseModel):
    response: str
    search_results: List[dict]
    memory_content: str
    timing: dict
    stats: dict

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    vector_db_connected: bool
    document_count: int
    uptime: str

# ì‹œì‘ ì‹œê°„ ê¸°ë¡
start_time = time.time()

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
    global kanana_model, vector_service, gpt_service
    
    print("ğŸš€ RAG Chat ë°±ì—”ë“œ ì„œë²„ ì´ˆê¸°í™” ì¤‘...")
    
    # í™˜ê²½ ì„¤ì • í™•ì¸
    settings.print_settings_summary()
    if not check_environment():
        print("âŒ í™˜ê²½ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit(1)
    
    try:
        # 1. Kanana ëª¨ë¸ ë¡œë”©
        print("ğŸ“¦ Kanana ëª¨ë¸ ë¡œë”©...")
        kanana_model = KananaModel(settings.get_model_config())
        await asyncio.to_thread(kanana_model.load_model)
        
        # 2. ë²¡í„° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        print("ğŸ—„ï¸ Vector ì„œë¹„ìŠ¤ ì´ˆê¸°í™”...")
        vector_service = VectorService(kanana_model, settings.get_chroma_config())
        await asyncio.to_thread(vector_service.initialize)
        
        # 3. GPT ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        print("ğŸ§  GPT ì„œë¹„ìŠ¤ ì´ˆê¸°í™”...")
        gpt_service = GPTService(settings.get_openai_config())
        
        print("âœ… ëª¨ë“  ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        raise e

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    uptime_seconds = time.time() - start_time
    uptime_str = f"{int(uptime_seconds // 3600)}h {int((uptime_seconds % 3600) // 60)}m {int(uptime_seconds % 60)}s"
    
    doc_count = 0
    vector_connected = False
    
    if vector_service:
        try:
            doc_count = vector_service.get_document_count()
            vector_connected = True
        except:
            pass
    
    return HealthResponse(
        status="healthy" if kanana_model and vector_service and gpt_service else "initializing",
        model_loaded=kanana_model is not None,
        vector_db_connected=vector_connected,
        document_count=doc_count,
        uptime=uptime_str
    )

@app.post("/chat", response_model=ChatResponse)
async def chat(message: ChatMessage):
    """ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬"""
    if not all([kanana_model, vector_service, gpt_service]):
        raise HTTPException(status_code=503, detail="Services not initialized")
    
    query_start_time = time.time()
    
    try:
        # í†µê³„ ì—…ë°ì´íŠ¸
        stats["total_queries"] += 1
        
        # 1. ë²¡í„° ê²€ìƒ‰
        search_start = time.time()
        search_results = await asyncio.to_thread(
            vector_service.search_similar,
            message.message,
            n_results=settings.search_default_results
        )
        search_time = (time.time() - search_start) * 1000
        stats["total_search_time"] += search_time
        
        # 2. ë‹¨ê¸° ê¸°ì–µ ê°€ì ¸ì˜¤ê¸°
        memory_content = memory.view()
        
        # 3. GPT ì‘ë‹µ ìƒì„±
        gpt_start = time.time()
        response = await gpt_service.generate_response(
            user_message=message.message,
            search_results=search_results,
            memory_content=memory_content
        )
        gpt_time = (time.time() - gpt_start) * 1000
        stats["total_gpt_time"] += gpt_time
        
        # 4. ë²¡í„°í™” ë° ì €ì¥
        embedding_start = time.time()
        doc = f"USER : {message.message}<\\n>ASSISTANT : {response}"
        await asyncio.to_thread(vector_service.add_document, doc)
        embedding_time = (time.time() - embedding_start) * 1000
        stats["total_embedding_time"] += embedding_time
        
        # 5. ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        memory.append({"role": "user", "content": message.message})
        memory.append({"role": "assistant", "content": response})
        
        # íƒ€ì´ë° ì •ë³´
        total_time = (time.time() - query_start_time) * 1000
        timing = {
            "total": f"{total_time:.2f}ms",
            "search": f"{search_time:.2f}ms",
            "gpt": f"{gpt_time:.2f}ms",
            "embedding": f"{embedding_time:.2f}ms"
        }
        
        # í‰ê·  í†µê³„ ê³„ì‚°
        avg_stats = {}
        if stats["total_queries"] > 0:
            avg_stats = {
                "total_queries": stats["total_queries"],
                "avg_search": f"{stats['total_search_time'] / stats['total_queries']:.2f}ms",
                "avg_gpt": f"{stats['total_gpt_time'] / stats['total_queries']:.2f}ms",
                "avg_embedding": f"{stats['total_embedding_time'] / stats['total_queries']:.2f}ms"
            }
        
        return ChatResponse(
            response=response,
            search_results=search_results,
            memory_content=memory_content,
            timing=timing,
            stats=avg_stats
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat processing error: {str(e)}")

@app.get("/memory")
async def get_memory():
    """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    return {
        "messages": memory.messages,
        "count": len(memory.messages),
        "max_count": memory.max_count
    }

@app.delete("/memory")
async def clear_memory():
    """ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
    memory.clear()
    return {"message": "Memory cleared successfully"}

@app.get("/stats")
async def get_stats():
    """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ"""
    avg_stats = {}
    if stats["total_queries"] > 0:
        avg_stats = {
            "total_queries": stats["total_queries"],
            "avg_search_time": f"{stats['total_search_time'] / stats['total_queries']:.2f}ms",
            "avg_gpt_time": f"{stats['total_gpt_time'] / stats['total_queries']:.2f}ms",
            "avg_embedding_time": f"{stats['total_embedding_time'] / stats['total_queries']:.2f}ms"
        }
    
    return {
        "raw_stats": stats,
        "averages": avg_stats,
        "document_count": vector_service.get_document_count() if vector_service else 0
    }

if __name__ == "__main__":
    print("ğŸš€ RAG Chat ë°±ì—”ë“œ ì„œë²„ ì‹œì‘!")
    uvicorn.run(
        "app:app",
        host=settings.host,
        port=settings.port,
        reload=settings.reload,
        log_level=settings.log_level.lower()
    )