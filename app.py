from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import uvicorn
import asyncio
import time
from datetime import datetime
import atexit

from config.settings import settings, check_environment
from models.kanana_model import KananaModel
from services.vector_service import VectorService
from services.gpt_service import GPTService
from utils.memory_manager import MessageQueue
from utils.mlops_manager import MLOpsManager  # ğŸš€ ìƒˆë¡œìš´ í†µí•© MLOps ë§¤ë‹ˆì €

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(title="RAG Chat API with Advanced MLOps", version="3.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜ë“¤
kanana_model = None
vector_service = None
gpt_service = None
memory = MessageQueue(cnt=settings.memory_max_count)
mlops_manager = None  # ğŸš€ í†µí•© MLOps ë§¤ë‹ˆì €

stats = {
    "total_queries": 0,
    "total_search_time": 0,
    "total_gpt_time": 0,
    "total_embedding_time": 0
}

# Pydantic ëª¨ë¸ë“¤
class ChatMessage(BaseModel):
    message: str
    user_id: Optional[str] = "default"
    session_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    search_results: List[dict]
    memory_content: str
    timing: dict
    stats: dict
    mlops_info: dict

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    vector_db_connected: bool
    document_count: int
    uptime: str
    mlops_status: dict

# ğŸš€ ìƒˆë¡œìš´ MLOps ê´€ë ¨ ëª¨ë¸ë“¤
class MLOpsStatusResponse(BaseModel):
    collector_stats: dict
    training_status: dict
    models_info: dict
    events_summary: dict
    performance_metrics: dict

class FinetuneRequest(BaseModel):
    force: bool = False
    backup_existing: bool = True

class FinetuneResponse(BaseModel):
    success: bool
    message: str
    version: Optional[str] = None
    training_data_count: int
    training_time: Optional[float] = None
    estimated_time: Optional[str] = None

class MLOpsSettingsRequest(BaseModel):
    batch_size: Optional[int] = None
    auto_trigger: Optional[bool] = None
    collection_enabled: Optional[bool] = None
    monitoring_enabled: Optional[bool] = None

# ì‹œì‘ ì‹œê°„ ê¸°ë¡
start_time = time.time()

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
    global kanana_model, vector_service, gpt_service, mlops_manager
    
    print("ğŸš€ RAG Chat ë°±ì—”ë“œ ì„œë²„ (Advanced MLOps) ì´ˆê¸°í™” ì¤‘...")
    
    # í™˜ê²½ ì„¤ì • í™•ì¸
    settings.print_settings_summary()
    if not check_environment():
        print("âŒ í™˜ê²½ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit(1)
    
    try:
        # 1. Kanana ëª¨ë¸ ë¡œë”©
        print("ğŸ“¦ Kanana ëª¨ë¸ ë¡œë”©...")
        kanana_model = KananaModel(settings.get_model_config())
        await asyncio.to_thread(kanana_model.load_model)
        
        # 2. ë²¡í„° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        print("ğŸ—„ï¸ Vector ì„œë¹„ìŠ¤ ì´ˆê¸°í™”...")
        vector_service = VectorService(kanana_model, settings.get_chroma_config())
        await asyncio.to_thread(vector_service.initialize)
        
        # 3. GPT ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        print("ğŸ§  GPT ì„œë¹„ìŠ¤ ì´ˆê¸°í™”...")
        gpt_service = GPTService(settings.get_openai_config())
        
        # ğŸš€ 4. í†µí•© MLOps ë§¤ë‹ˆì € ì´ˆê¸°í™”
        print("ğŸ¤– í†µí•© MLOps ë§¤ë‹ˆì € ì´ˆê¸°í™”...")
        mlops_manager = MLOpsManager(
            finetune_config=settings.get_finetune_config(),
            conversation_config=settings.get_conversation_config()
        )
        
        print("âœ… ëª¨ë“  ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")
        
        # ğŸš€ ì„œë²„ ì‹œì‘ ì´ë²¤íŠ¸ ë¡œê¹…
        if mlops_manager:
            mlops_manager._log_event("server_started", {
                "model_loaded": True,
                "vector_db_connected": True,
                "gpt_service_ready": True
            }, "RAG Chat ì„œë²„ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        raise e

@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‘ì—…"""
    print("ğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")
    
    if mlops_manager:
        mlops_manager.shutdown()
    
    print("âœ… ì •ë¦¬ ì‘ì—… ì™„ë£Œ")

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ì •ë¦¬
atexit.register(lambda: mlops_manager.shutdown() if mlops_manager else None)

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸ (MLOps í¬í•¨)"""
    uptime_seconds = time.time() - start_time
    uptime_str = f"{int(uptime_seconds // 3600)}h {int((uptime_seconds % 3600) // 60)}m {int(uptime_seconds % 60)}s"
    
    doc_count = 0
    vector_connected = False
    
    if vector_service:
        try:
            doc_count = vector_service.get_document_count()
            vector_connected = True
        except:
            pass
    
    # ğŸš€ MLOps ìƒíƒœ ì •ë³´
    mlops_status = {}
    if mlops_manager:
        try:
            mlops_status = mlops_manager.get_status()
        except Exception as e:
            mlops_status = {"error": str(e)}
    
    return HealthResponse(
        status="healthy" if kanana_model and vector_service and gpt_service else "initializing",
        model_loaded=kanana_model is not None,
        vector_db_connected=vector_connected,
        document_count=doc_count,
        uptime=uptime_str,
        mlops_status=mlops_status
    )

@app.post("/chat", response_model=ChatResponse)
async def chat(message: ChatMessage):
    """ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬ (MLOps ëŒ€í™” ìˆ˜ì§‘ í¬í•¨)"""
    if not all([kanana_model, vector_service, gpt_service]):
        raise HTTPException(status_code=503, detail="Services not initialized")
    
    query_start_time = time.time()
    
    try:
        # í†µê³„ ì—…ë°ì´íŠ¸
        stats["total_queries"] += 1
        
        # 1. ë²¡í„° ê²€ìƒ‰
        search_start = time.time()
        search_results = await asyncio.to_thread(
            vector_service.search_similar,
            message.message,
            n_results=settings.search_default_results
        )
        search_time = (time.time() - search_start) * 1000
        stats["total_search_time"] += search_time
        
        # 2. ë‹¨ê¸° ê¸°ì–µ ê°€ì ¸ì˜¤ê¸°
        memory_content = memory.view()
        
        # 3. GPT ì‘ë‹µ ìƒì„±
        gpt_start = time.time()
        response = await gpt_service.generate_response(
            user_message=message.message,
            search_results=search_results,
            memory_content=memory_content
        )
        gpt_time = (time.time() - gpt_start) * 1000
        stats["total_gpt_time"] += gpt_time
        
        # ğŸš€ 4. MLOps ëŒ€í™” ìˆ˜ì§‘ ë° íŒŒì¸íŠœë‹ íŠ¸ë¦¬ê±° í™•ì¸
        mlops_info = {"collection_enabled": False, "training_triggered": False}
        if mlops_manager:
            try:
                mlops_result = mlops_manager.process_conversation(
                    user_message=message.message,
                    assistant_response=response,
                    user_id=message.user_id,
                    session_id=message.session_id,
                    metadata={
                        "search_results_count": len(search_results),
                        "memory_length": len(memory_content),
                        "timing": {
                            "search_ms": search_time,
                            "gpt_ms": gpt_time
                        }
                    }
                )
                mlops_info = mlops_result
            except Exception as e:
                print(f"âš ï¸ MLOps ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                mlops_info = {"error": str(e)}
        
        # 5. ë²¡í„°í™” ë° ì €ì¥
        embedding_start = time.time()
        doc = f"USER : {message.message}<\\n>ASSISTANT : {response}"
        await asyncio.to_thread(vector_service.add_document, doc)
        embedding_time = (time.time() - embedding_start) * 1000
        stats["total_embedding_time"] += embedding_time
        
        # 6. ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        memory.append({"role": "user", "content": message.message})
        memory.append({"role": "assistant", "content": response})
        
        # íƒ€ì´ë° ì •ë³´
        total_time = (time.time() - query_start_time) * 1000
        timing = {
            "total": f"{total_time:.2f}ms",
            "search": f"{search_time:.2f}ms",
            "gpt": f"{gpt_time:.2f}ms",
            "embedding": f"{embedding_time:.2f}ms"
        }
        
        # í‰ê·  í†µê³„ ê³„ì‚°
        avg_stats = {}
        if stats["total_queries"] > 0:
            avg_stats = {
                "total_queries": stats["total_queries"],
                "avg_search": f"{stats['total_search_time'] / stats['total_queries']:.2f}ms",
                "avg_gpt": f"{stats['total_gpt_time'] / stats['total_queries']:.2f}ms",
                "avg_embedding": f"{stats['total_embedding_time'] / stats['total_queries']:.2f}ms"
            }
        
        return ChatResponse(
            response=response,
            search_results=search_results,
            memory_content=memory_content,
            timing=timing,
            stats=avg_stats,
            mlops_info=mlops_info
        )
        
    except Exception as e:
        # ğŸš€ ì˜¤ë¥˜ ì´ë²¤íŠ¸ ë¡œê¹…
        if mlops_manager:
            mlops_manager._log_event("chat_error", {
                "error": str(e),
                "user_id": message.user_id,
                "message_length": len(message.message)
            }, f"ì±„íŒ… ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        
        raise HTTPException(status_code=500, detail=f"Chat processing error: {str(e)}")

# ğŸš€ ê°•í™”ëœ MLOps ì—”ë“œí¬ì¸íŠ¸ë“¤

@app.get("/mlops/status", response_model=MLOpsStatusResponse)
async def get_mlops_status():
    """í†µí•© MLOps ìƒíƒœ ì¡°íšŒ"""
    if not mlops_manager:
        raise HTTPException(status_code=503, detail="MLOps manager not initialized")
    
    try:
        status = mlops_manager.get_status()
        performance = mlops_manager.get_performance_metrics()
        
        return MLOpsStatusResponse(
            collector_stats=status["collector"],
            training_status=status["training"],
            models_info=status["models"],
            events_summary=status["events"],
            performance_metrics=performance
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"MLOps status error: {str(e)}")

@app.get("/mlops/conversations")
async def get_conversations(limit: Optional[int] = 50):
    """ìˆ˜ì§‘ëœ ëŒ€í™” ëª©ë¡ ì¡°íšŒ"""
    if not mlops_manager:
        raise HTTPException(status_code=503, detail="MLOps manager not initialized")
    
    try:
        conversations = mlops_manager.collector.get_collected_conversations(limit)
        return {
            "conversations": [conv.to_dict() for conv in conversations],
            "total_count": mlops_manager.collector.stats["total_collected"],
            "stats": mlops_manager.collector.get_stats()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Conversations retrieval error: {str(e)}")

@app.post("/mlops/finetune", response_model=FinetuneResponse)
async def trigger_finetuning(request: FinetuneRequest, background_tasks: BackgroundTasks):
    """ìˆ˜ë™ íŒŒì¸íŠœë‹ íŠ¸ë¦¬ê±°"""
    if not mlops_manager:
        raise HTTPException(status_code=503, detail="MLOps manager not initialized")
    
    if mlops_manager.training_in_progress:
        raise HTTPException(status_code=409, detail="Training already in progress")
    
    try:
        # ìˆ˜ì§‘ëœ ëŒ€í™” ìˆ˜ í™•ì¸
        total_conversations = mlops_manager.collector.stats["total_collected"]
        
        if total_conversations == 0:
            return FinetuneResponse(
                success=False,
                message="ìˆ˜ì§‘ëœ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.",
                training_data_count=0
            )
        
        if not request.force and total_conversations < mlops_manager.batch_size:
            return FinetuneResponse(
                success=False,
                message=f"ë°°ì¹˜ í¬ê¸°({mlops_manager.batch_size})ì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. force=trueë¡œ ê°•ì œ ì‹¤í–‰ ê°€ëŠ¥.",
                training_data_count=total_conversations
            )
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ íŒŒì¸íŠœë‹ ì‹¤í–‰
        def run_training():
            try:
                mlops_manager._log_event("training_triggered", {
                    "trigger_type": "manual",
                    "total_conversations": total_conversations,
                    "force": request.force
                }, "ìˆ˜ë™ íŒŒì¸íŠœë‹ íŠ¸ë¦¬ê±°")
                
                result = mlops_manager.start_finetuning(force=request.force)
                
                if result["success"]:
                    mlops_manager.current_model_version = result["version"]
                    mlops_manager.last_training_count = total_conversations
                    
                    mlops_manager._log_event("training_completed", {
                        "version": result["version"],
                        "training_time": result["training_time"],
                        "training_samples": result["training_samples"]
                    }, f"ìˆ˜ë™ íŒŒì¸íŠœë‹ ì™„ë£Œ! ë²„ì „: {result['version']}")
                
            except Exception as e:
                mlops_manager._log_event("training_failed", {
                    "error": str(e),
                    "trigger_type": "manual"
                }, f"ìˆ˜ë™ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: {str(e)}")
        
        # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
        background_tasks.add_task(run_training)
        
        return FinetuneResponse(
            success=True,
            message="íŒŒì¸íŠœë‹ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            training_data_count=total_conversations,
            estimated_time="ì•½ 5-10ë¶„"
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Finetuning error: {str(e)}")

@app.delete("/mlops/conversations")
async def clear_conversations(backup: bool = True):
    """ìˆ˜ì§‘ëœ ëŒ€í™” ë°ì´í„° ì´ˆê¸°í™”"""
    if not mlops_manager:
        raise HTTPException(status_code=503, detail="MLOps manager not initialized")
    
    try:
        old_count = mlops_manager.collector.stats["total_collected"]
        success = mlops_manager.collector.clear_conversations(backup_first=backup)
        
        if success:
            mlops_manager._log_event("conversations_cleared", {
                "cleared_count": old_count,
                "backup_created": backup
            }, f"ëŒ€í™” ë°ì´í„° ì´ˆê¸°í™”: {old_count}ê°œ ì œê±°")
        
        return {
            "success": success,
            "message": "ëŒ€í™” ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤." if success else "ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
            "backup_created": backup and success,
            "cleared_count": old_count if success else 0
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Clear conversations error: {str(e)}")

@app.post("/mlops/export")
async def export_training_data():
    """íŒŒì¸íŠœë‹ìš© ë°ì´í„°ì…‹ ìƒì„± ë° ë‚´ë³´ë‚´ê¸°"""
    if not mlops_manager:
        raise HTTPException(status_code=503, detail="MLOps manager not initialized")
    
    try:
        dataset_path = mlops_manager.collector.export_for_finetuning()
        
        if dataset_path:
            mlops_manager._log_event("dataset_exported", {
                "dataset_path": dataset_path,
                "sample_count": mlops_manager.collector.stats["total_collected"]
            }, "í•™ìŠµ ë°ì´í„°ì…‹ì´ ë‚´ë³´ë‚´ì¡ŒìŠµë‹ˆë‹¤.")
            
            return {
                "success": True,
                "message": "ë°ì´í„°ì…‹ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "dataset_path": dataset_path,
                "training_samples": mlops_manager.collector.stats["total_collected"]
            }
        else:
            return {
                "success": False,
                "message": "ë°ì´í„°ì…‹ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "dataset_path": None,
                "training_samples": 0
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Export error: {str(e)}")

@app.get("/mlops/training-progress")
async def get_training_progress():
    """íŒŒì¸íŠœë‹ ì§„í–‰ ìƒí™© ì¡°íšŒ"""
    if not mlops_manager:
        raise HTTPException(status_code=503, detail="MLOps manager not initialized")
    
    current_count = mlops_manager.collector.stats["total_collected"]
    batch_size = mlops_manager.batch_size
    
    return {
        "current_conversations": current_count,
        "batch_size": batch_size,
        "progress_percentage": (current_count % batch_size) / batch_size * 100 if batch_size > 0 else 0,
        "conversations_until_training": mlops_manager.collector.get_pending_training_count(batch_size),
        "training_in_progress": mlops_manager.training_in_progress,
        "auto_trigger_enabled": mlops_manager.auto_trigger,
        "current_version": mlops_manager.current_model_version,
        "last_training_count": mlops_manager.last_training_count
    }

@app.post("/mlops/settings")
async def update_mlops_settings(settings_request: MLOpsSettingsRequest):
    """MLOps ì„¤ì • ë™ì  ì—…ë°ì´íŠ¸"""
    if not mlops_manager:
        raise HTTPException(status_code=503, detail="MLOps manager not initialized")
    
    try:
        # ì„¤ì • ì—…ë°ì´íŠ¸
        updates = {}
        if settings_request.batch_size is not None:
            updates["batch_size"] = settings_request.batch_size
        if settings_request.auto_trigger is not None:
            updates["auto_trigger"] = settings_request.auto_trigger
        if settings_request.collection_enabled is not None:
            updates["collection_enabled"] = settings_request.collection_enabled
        if settings_request.monitoring_enabled is not None:
            updates["monitoring_enabled"] = settings_request.monitoring_enabled
        
        updated = mlops_manager.update_settings(**updates)
        
        return {
            "success": True,
            "message": "ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "updated_settings": updated
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Settings update error: {str(e)}")

@app.get("/mlops/events")
async def get_mlops_events(
    event_type: Optional[str] = None,
    limit: int = 100
):
    """MLOps ì´ë²¤íŠ¸ ë¡œê·¸ ì¡°íšŒ"""
    if not mlops_manager:
        raise HTTPException(status_code=503, detail="MLOps manager not initialized")
    
    try:
        events = mlops_manager.get_events_log(event_type=event_type, limit=limit)
        
        return {
            "events": events,
            "total_count": len(mlops_manager.events_log),
            "filtered_count": len(events),
            "filter": event_type
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Events retrieval error: {str(e)}")

@app.get("/mlops/training-history")
async def get_training_history():
    """íŒŒì¸íŠœë‹ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    if not mlops_manager:
        raise HTTPException(status_code=503, detail="MLOps manager not initialized")
    
    try:
        history = mlops_manager.get_training_history()
        
        return {
            "training_history": history,
            "total_trainings": len(history),
            "successful_trainings": len([h for h in history if h.get("success", False)])
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Training history error: {str(e)}")

@app.get("/mlops/models")
async def get_model_versions():
    """ëª¨ë¸ ë²„ì „ ëª©ë¡ ì¡°íšŒ"""
    if not mlops_manager or not mlops_manager.finetuner:
        raise HTTPException(status_code=503, detail="Finetuner not available")
    
    try:
        versions = mlops_manager.finetuner.get_model_versions()
        
        return {
            "model_versions": versions,
            "total_versions": len(versions),
            "current_version": mlops_manager.current_model_version,
            "latest_version": versions[0]["version"] if versions else None
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Model versions error: {str(e)}")

@app.get("/mlops/performance")
async def get_performance_metrics():
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    if not mlops_manager:
        raise HTTPException(status_code=503, detail="MLOps manager not initialized")
    
    try:
        metrics = mlops_manager.get_performance_metrics()
        
        return {
            "performance_metrics": metrics,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Performance metrics error: {str(e)}")

@app.post("/mlops/cleanup")
async def cleanup_old_data(keep_days: int = 30):
    """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
    if not mlops_manager:
        raise HTTPException(status_code=503, detail="MLOps manager not initialized")
    
    try:
        result = mlops_manager.cleanup_old_data(keep_days=keep_days)
        
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Cleanup error: {str(e)}")

# ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ë“¤ ìœ ì§€

@app.get("/memory")
async def get_memory():
    """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    return {
        "messages": memory.messages,
        "count": len(memory.messages),
        "max_count": memory.max_count
    }

@app.delete("/memory")
async def clear_memory():
    """ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
    memory.clear()
    
    # ğŸš€ ì´ë²¤íŠ¸ ë¡œê¹…
    if mlops_manager:
        mlops_manager._log_event("memory_cleared", {
            "previous_count": len(memory.messages)
        }, "ëŒ€í™” ë©”ëª¨ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return {"message": "Memory cleared successfully"}

@app.get("/stats")
async def get_stats():
    """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ (MLOps í¬í•¨)"""
    avg_stats = {}
    if stats["total_queries"] > 0:
        avg_stats = {
            "total_queries": stats["total_queries"],
            "avg_search_time": f"{stats['total_search_time'] / stats['total_queries']:.2f}ms",
            "avg_gpt_time": f"{stats['total_gpt_time'] / stats['total_queries']:.2f}ms",
            "avg_embedding_time": f"{stats['total_embedding_time'] / stats['total_queries']:.2f}ms"
        }
    
    # ğŸš€ MLOps í†µê³„ ì¶”ê°€
    mlops_stats = {}
    if mlops_manager:
        try:
            mlops_stats = mlops_manager.get_status()
        except Exception as e:
            mlops_stats = {"error": str(e)}
    
    return {
        "performance": {
            "raw_stats": stats,
            "averages": avg_stats,
            "document_count": vector_service.get_document_count() if vector_service else 0
        },
        "mlops": mlops_stats,
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    print("ğŸš€ RAG Chat ë°±ì—”ë“œ ì„œë²„ (Advanced MLOps) ì‹œì‘!")
    print("=" * 80)
    print("ğŸ†• ìƒˆë¡œìš´ Advanced MLOps ì—”ë“œí¬ì¸íŠ¸:")
    print("- GET  /mlops/status            : í†µí•© MLOps ìƒíƒœ (ì„±ëŠ¥ ë©”íŠ¸ë¦­ í¬í•¨)")
    print("- GET  /mlops/conversations     : ìˆ˜ì§‘ëœ ëŒ€í™” ì¡°íšŒ")
    print("- POST /mlops/finetune          : ìˆ˜ë™ íŒŒì¸íŠœë‹ íŠ¸ë¦¬ê±° (ë°±ê·¸ë¼ìš´ë“œ)")
    print("- DELETE /mlops/conversations   : ëŒ€í™” ë°ì´í„° ì´ˆê¸°í™”")
    print("- POST /mlops/export           : í•™ìŠµ ë°ì´í„° ë‚´ë³´ë‚´ê¸°")
    print("- GET  /mlops/training-progress : í•™ìŠµ ì§„í–‰ ìƒí™©")
    print("- POST /mlops/settings         : ì„¤ì • ë™ì  ì—…ë°ì´íŠ¸")
    print("- GET  /mlops/events           : ì´ë²¤íŠ¸ ë¡œê·¸ ì¡°íšŒ")
    print("- GET  /mlops/training-history : íŒŒì¸íŠœë‹ íˆìŠ¤í† ë¦¬")
    print("- GET  /mlops/models           : ëª¨ë¸ ë²„ì „ ëª©ë¡")
    print("- GET  /mlops/performance      : ì„±ëŠ¥ ë©”íŠ¸ë¦­")
    print("- POST /mlops/cleanup          : ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬")
    print("=" * 80)
    print("ğŸ¯ í•µì‹¬ ê¸°ëŠ¥:")
    print("- ğŸ¤– ìë™ ëŒ€í™” ìˆ˜ì§‘ ë° íŒŒì¸íŠœë‹")
    print("- ğŸ“Š ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
    print("- ğŸ”„ ì ì§„ì  ëª¨ë¸ ê°œì„  (v1 â†’ v2 â†’ v3)")
    print("- ğŸ“ˆ ì´ë²¤íŠ¸ ë¡œê¹… ë° ì›¹í›… ì•Œë¦¼")
    print("- âš¡ ë°±ê·¸ë¼ìš´ë“œ íŒŒì¸íŠœë‹")
    print("=" * 80)
    
    uvicorn.run(
        "app:app",
        host=settings.host,
        port=settings.port,
        reload=settings.reload,
        log_level=settings.log_level.lower()
    )