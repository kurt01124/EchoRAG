import torch
import time
import base64
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

class KananaModel:
    """Kanana ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: dict):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        if config.get("device") == "cpu":
            self.device = "cpu"
        elif config.get("device") == "cuda":
            self.device = "cuda"
        
        self.model_name = config.get("model_name", "kakaocorp/kanana-1.5-2.1b-instruct-2505")
        self.finetuned_path = config.get("finetuned_path", "./kanana-vector-restoration")
        self.dtype_str = config.get("dtype", "bfloat16")
        
        # dtype ì„¤ì •
        if self.dtype_str == "bfloat16":
            self.dtype = torch.bfloat16
        elif self.dtype_str == "float16":
            self.dtype = torch.float16
        else:
            self.dtype = torch.float32
        
        self.tokenizer = None
        self.base_model = None
        self.model = None
        self.embedding_layer = None
        
        print(f"ğŸ”¥ ë””ë°”ì´ìŠ¤: {self.device}, ë°ì´í„° íƒ€ì…: {self.dtype_str}")
    
    def load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
        print("ğŸš€ Kanana ëª¨ë¸ ë¡œë”© ì‹œì‘...")
        start_time = time.time()
        
        try:
            # 1. í† í¬ë‚˜ì´ì € ë¡œë”©
            print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”©...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # 2. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©
            print("ğŸ§  ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©...")
            self.base_model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=self.dtype,
                trust_remote_code=True
            ).eval().to(self.device)
            
            # 3. íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë”© (ìˆëŠ” ê²½ìš°)
            try:
                print("ğŸ¯ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë”©...")
                self.model = PeftModel.from_pretrained(self.base_model, self.finetuned_path)
                print("âœ… íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                print("ğŸ“¦ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ì§„í–‰...")
                self.model = self.base_model
            
            # 4. ì„ë² ë”© ë ˆì´ì–´ ì„¤ì •
            self.embedding_layer = self.model.get_input_embeddings()
            
            load_time = time.time() - start_time
            print(f"âœ… Kanana ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {load_time:.2f}ì´ˆ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e
    
    def embed_optimized(self, text: str):
        """bfloat16ì„ ìœ ì§€í•˜ëŠ” ìµœì í™”ëœ ì„ë² ë”© í•¨ìˆ˜"""
        if not self.tokenizer or not self.embedding_layer:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        ids = self.tokenizer(text, return_tensors="pt", add_special_tokens=False)["input_ids"].to(self.device)
        vectors = self.embedding_layer(ids).squeeze(0)
        result_bfloat16 = vectors.mean(dim=0).detach()  # bfloat16 ìœ ì§€
        return result_bfloat16
    
    def embed(self, text: str):
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ float32 í•¨ìˆ˜ (ChromaDB ê²€ìƒ‰ìš©)"""
        bfloat16_result = self.embed_optimized(text)
        float32_list = self.bfloat16_to_float32_list(bfloat16_result)
        return float32_list
    
    def embed_text(self, text: str):
        """í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        ids = self.tokenizer(text, return_tensors="pt", add_special_tokens=False)["input_ids"].to(self.device)
        result = self.embedding_layer(ids)
        return result
    
    def embed_token(self, token: str):
        """í† í° ì„ë² ë”©"""
        token_id = self.tokenizer.convert_tokens_to_ids(token)
        result = self.embedding_layer(torch.tensor([[token_id]]).to(self.device))
        return result
    
    def embed_sentence(self, sentence_text: str):
        """ë¬¸ì¥ ì „ì²´ ì„ë² ë”© (ì±„íŒ… í˜•ì‹)"""
        bos               = self.embed_token("<|begin_of_text|>")
        sys_head          = self.embed_token("<|start_header_id|>")
        sys_role          = self.embed_text("system")
        sys_tail          = self.embed_token("<|end_header_id|>")
        sys_content       = self.embed_text("ë‹¹ì‹ ì€ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì½ì–´ì£¼ëŠ” ì¹œì ˆí•œ AI ë¹„ì„œì…ë‹ˆë‹¤.")
        sys_eot           = self.embed_token("<|eot_id|>")
        user_head         = self.embed_token("<|start_header_id|>")
        user_role         = self.embed_text("user")
        user_tail         = self.embed_token("<|end_header_id|>")
        sentence_embed    = self.embed_text(sentence_text)
        suffix_embed      = self.embed_text("ì´ ë¬¸ì¥ì„ ìµœëŒ€í•œ ì›ë³¸ê³¼ ë˜‘ê°™ì´ ì½ì–´ì£¼ì„¸ìš”.")
        user_eot          = self.embed_token("<|eot_id|>")
        asst_head         = self.embed_token("<|start_header_id|>")
        asst_role         = self.embed_text("assistant")
        asst_tail         = self.embed_token("<|end_header_id|>")
        
        result = torch.cat([
            bos,
            sys_head, sys_role, sys_tail,
            sys_content, sys_eot,
            user_head, user_role, user_tail,
            sentence_embed, suffix_embed, user_eot,
            asst_head, asst_role, asst_tail
        ], dim=1)
        
        return result
    
    def generate_text(self, input_text: str, max_new_tokens: int = 100):
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        if not self.tokenizer or not self.model:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        inputs = self.tokenizer(input_text, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ê²°ê³¼ ì¶”ì¶œ
        if "assistant" in response:
            result = response.split("assistant")[-1].strip()
        else:
            result = response.strip()
        
        return result
    
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    @staticmethod
    def serialize_bfloat16_vector(tensor):
        """bfloat16 í…ì„œë¥¼ ChromaDB ì €ì¥ìš© ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if tensor.dtype != torch.bfloat16:
            tensor = tensor.to(torch.bfloat16)
        
        bytes_data = tensor.cpu().numpy().tobytes()
        return base64.b64encode(bytes_data).decode('ascii')
    
    @staticmethod
    def deserialize_bfloat16_vector(encoded_str, shape):
        """ChromaDBì—ì„œ ê°€ì ¸ì˜¨ ë¬¸ìì—´ì„ bfloat16 í…ì„œë¡œ ë³µì›"""
        bytes_data = base64.b64decode(encoded_str.encode('ascii'))
        numpy_array = np.frombuffer(bytes_data, dtype=np.uint16)
        
        tensor = torch.from_numpy(numpy_array.view(np.dtype('>u2'))).view(torch.bfloat16)
        return tensor.reshape(shape)
    
    @staticmethod
    def bfloat16_to_float32_list(tensor):
        """bfloat16 í…ì„œë¥¼ ChromaDB ê²€ìƒ‰ìš© float32 ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        return tensor.to(torch.float32).cpu().numpy().tolist()
    
    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if not self.model:
            return {"status": "not_loaded"}
        
        return {
            "status": "loaded",
            "model_name": self.model_name,
            "device": self.device,
            "finetuned_path": self.finetuned_path,
            "has_finetuned": isinstance(self.model, PeftModel),
            "parameters": self.model.num_parameters() if hasattr(self.model, 'num_parameters') else "unknown",
            "vocab_size": len(self.tokenizer) if self.tokenizer else "unknown"
        }