import torch
import json
import os
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
)
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from datasets import Dataset

class CustomDataCollator:
    """ì»¤ìŠ¤í…€ ë°ì´í„° ì½œë ˆì´í„°"""
    def __init__(self, tokenizer, pad_to_multiple_of=None):
        self.tokenizer = tokenizer
        self.pad_to_multiple_of = pad_to_multiple_of
    
    def __call__(self, features):
        input_ids = [f["input_ids"] for f in features]
        labels = [f["labels"] for f in features]
        
        max_length = max(len(ids) for ids in input_ids)
        if self.pad_to_multiple_of:
            max_length = ((max_length + self.pad_to_multiple_of - 1) // self.pad_to_multiple_of) * self.pad_to_multiple_of
        
        padded_input_ids = []
        padded_labels = []
        attention_masks = []
        
        for ids, lbls in zip(input_ids, labels):
            padding_length = max_length - len(ids)
            
            padded_ids = ids + [self.tokenizer.pad_token_id] * padding_length
            padded_input_ids.append(padded_ids)
            
            padded_lbls = lbls + [-100] * padding_length
            padded_labels.append(padded_lbls)
            
            attention_mask = [1] * len(ids) + [0] * padding_length
            attention_masks.append(attention_mask)
        
        return {
            "input_ids": torch.tensor(padded_input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_masks, dtype=torch.long),
            "labels": torch.tensor(padded_labels, dtype=torch.long),
        }

class AutomatedFinetuner:
    """ìë™í™”ëœ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # ê¸°ë³¸ ì„¤ì •
        self.model_name = config.get("model_name", "kakaocorp/kanana-1.5-2.1b-instruct-2505")
        self.base_output_dir = Path(config.get("models_path", "./models"))
        self.data_path = Path(config.get("data_path", "./data/finetune"))
        self.backup_count = config.get("backup_count", 3)
        self.version_prefix = config.get("version_prefix", "v")
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.hyperparams = config.get("hyperparameters", {})
        self.epochs = self.hyperparams.get("epochs", 1)
        self.learning_rate = self.hyperparams.get("learning_rate", 2e-5)
        self.lora_r = self.hyperparams.get("lora_r", 8)
        self.lora_alpha = self.hyperparams.get("lora_alpha", 16)
        self.lora_dropout = self.hyperparams.get("lora_dropout", 0.1)
        
        # ìƒíƒœ ì¶”ì 
        self.current_version = self._get_next_version()
        self.training_log = []
        
        # ë””ë ‰í„°ë¦¬ ìƒì„±
        self.base_output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ¤– ìë™í™”ëœ íŒŒì¸íŠœë„ˆ ì´ˆê¸°í™”")
        print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {self.base_output_dir}")
        print(f"ğŸ·ï¸ ë‹¤ìŒ ë²„ì „: {self.current_version}")
        print(f"ğŸ›ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°: epochs={self.epochs}, lr={self.learning_rate}, r={self.lora_r}")
    
    def _get_next_version(self) -> str:
        """ë‹¤ìŒ ë²„ì „ ë²ˆí˜¸ ê³„ì‚°"""
        existing_versions = []
        
        # ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í„°ë¦¬ì—ì„œ ë²„ì „ ë²ˆí˜¸ ì°¾ê¸°
        if self.base_output_dir.exists():
            for item in self.base_output_dir.iterdir():
                if item.is_dir() and item.name.startswith(self.version_prefix):
                    try:
                        version_num = int(item.name[len(self.version_prefix):])
                        existing_versions.append(version_num)
                    except ValueError:
                        continue
        
        if existing_versions:
            next_version = max(existing_versions) + 1
        else:
            next_version = 1
        
        return f"{self.version_prefix}{next_version}"
    
    def _get_latest_model_path(self) -> Optional[Path]:
        """ìµœì‹  ëª¨ë¸ ê²½ë¡œ ë°˜í™˜"""
        if not self.base_output_dir.exists():
            return None
        
        latest_version = 0
        latest_path = None
        
        for item in self.base_output_dir.iterdir():
            if item.is_dir() and item.name.startswith(self.version_prefix):
                try:
                    version_num = int(item.name[len(self.version_prefix):])
                    if version_num > latest_version:
                        latest_version = version_num
                        latest_path = item
                except ValueError:
                    continue
        
        return latest_path
    
    def _backup_existing_models(self):
        """ê¸°ì¡´ ëª¨ë¸ë“¤ ë°±ì—… ê´€ë¦¬"""
        models = []
        
        # ëª¨ë“  ë²„ì „ ëª¨ë¸ ì°¾ê¸°
        for item in self.base_output_dir.iterdir():
            if item.is_dir() and item.name.startswith(self.version_prefix):
                try:
                    version_num = int(item.name[len(self.version_prefix):])
                    models.append((version_num, item))
                except ValueError:
                    continue
        
        # ë²„ì „ ìˆœìœ¼ë¡œ ì •ë ¬
        models.sort(key=lambda x: x[0])
        
        # ë°±ì—… ê°œìˆ˜ ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ ëª¨ë¸ ì‚­ì œ
        while len(models) >= self.backup_count:
            old_version, old_path = models.pop(0)
            if old_path.exists():
                print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ëª¨ë¸ ì‚­ì œ: {old_path.name}")
                shutil.rmtree(old_path)
    
    def _convert_conversations_to_training_format(self, dataset_path: str) -> str:
        """ëŒ€í™” ë°ì´í„°ë¥¼ íŒŒì¸íŠœë‹ í˜•íƒœë¡œ ë³€í™˜"""
        print(f"ğŸ”„ í•™ìŠµ ë°ì´í„° ë³€í™˜: {dataset_path}")
        
        with open(dataset_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•íƒœë¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if isinstance(data, list) and len(data) > 0:
            if 'input' in data[0] and 'output' in data[0]:
                print("âœ… ì´ë¯¸ ì˜¬ë°”ë¥¸ í•™ìŠµ ë°ì´í„° í˜•íƒœì…ë‹ˆë‹¤.")
                return dataset_path
        
        # ëŒ€í™” í˜•íƒœë¼ë©´ ë³€í™˜
        training_data = []
        
        for item in data:
            if isinstance(item, dict):
                if 'to_training_format' in item:
                    # ConversationEntry í˜•íƒœ
                    conversation_text = item['to_training_format']()
                elif 'user_message' in item and 'assistant_response' in item:
                    # ì§ì ‘ ëŒ€í™” í˜•íƒœ
                    conversation_text = f"USER : {item['user_message']}<\\n>ASSISTANT : {item['assistant_response']}"
                else:
                    continue
                
                # ì˜¬ë°”ë¥¸ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                training_sample = {
                    "text": self._create_chat_format(conversation_text),
                    "metadata": item.get('metadata', {})
                }
                
                training_data.append(training_sample)
        
        # ë³€í™˜ëœ ë°ì´í„° ì €ì¥
        converted_path = self.data_path / f"converted_training_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(converted_path, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ë³€í™˜ ì™„ë£Œ: {len(training_data)}ê°œ ìƒ˜í”Œ â†’ {converted_path}")
        return str(converted_path)
    
    def _create_chat_format(self, conversation_text: str) -> str:
        """ëŒ€í™”ë¥¼ ì˜¬ë°”ë¥¸ ì±„íŒ… í˜•íƒœë¡œ ë³€í™˜"""
        return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì½ì–´ì£¼ëŠ” ì¹œì ˆí•œ AI ë¹„ì„œì…ë‹ˆë‹¤.<|eot_id|><|start_header_id|>user<|end_header_id|>

<TARGET>{conversation_text}</TARGET>TARGET íƒœê·¸ ì•ˆì˜ ë‚´ìš©ë§Œ ì¶œë ¥í•˜ì„¸ìš”.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{conversation_text}<|eot_id|>"""
    
    def _setup_model_and_tokenizer(self, existing_adapter_path: Optional[str] = None):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì • (ê¸°ì¡´ ì–´ëŒ‘í„° ë¡œë“œ í¬í•¨)"""
        print(f"ğŸš€ ëª¨ë¸ ë¡œë”©: {self.model_name}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            device_map="auto",
            use_cache=False,
        )
        
        # ê¸°ì¡´ ì–´ëŒ‘í„°ê°€ ìˆë‹¤ë©´ ë¡œë“œ
        if existing_adapter_path and Path(existing_adapter_path).exists():
            print(f"ğŸ”„ ê¸°ì¡´ ì–´ëŒ‘í„° ë¡œë“œ: {existing_adapter_path}")
            try:
                model = PeftModel.from_pretrained(base_model, existing_adapter_path)
                print("âœ… ê¸°ì¡´ ì–´ëŒ‘í„° ë¡œë“œ ì„±ê³µ - ì ì§„ì  í•™ìŠµ ì§„í–‰")
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ğŸ“¦ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ìƒˆë¡œ ì‹œì‘")
                model = base_model
        else:
            print("ğŸ“¦ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ìƒˆë¡œ ì‹œì‘")
            model = base_model
        
        print(f"ğŸ’¾ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}")
        return model, tokenizer
    
    def _setup_lora_config(self):
        """LoRA ì„¤ì •"""
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=self.lora_r,
            lora_alpha=self.lora_alpha,
            lora_dropout=self.lora_dropout,
            target_modules=[
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj",
            ],
            bias="none",
        )
        
        print("ğŸ¯ LoRA ì„¤ì •:")
        print(f"- Rank (r): {lora_config.r}")
        print(f"- Alpha: {lora_config.lora_alpha}")
        print(f"- Dropout: {lora_config.lora_dropout}")
        
        return lora_config
    
    def _load_and_prepare_dataset(self, tokenizer, dataset_file: str):
        """ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ - ì™„ì „íˆ ì˜¬ë°”ë¥¸ ë¼ë²¨ ë§ˆìŠ¤í‚¹"""
        print(f"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”©: {dataset_file}")
        
        with open(dataset_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ğŸ“Š ì´ {len(data)}ê°œì˜ í•™ìŠµ ìƒ˜í”Œ")
        
        dataset = Dataset.from_list(data)
        
        def tokenize_function(examples):
            """ğŸ”¥ ì™„ì „íˆ ì˜¬ë°”ë¥¸ í† í¬ë‚˜ì´ì§• ë° ë¼ë²¨ ë§ˆìŠ¤í‚¹"""
            model_inputs = tokenizer(
                examples["text"],
                truncation=True,
                padding=False,
                max_length=512,
                return_overflowing_tokens=False,
            )
            
            labels = []
            
            for i, text in enumerate(examples["text"]):
                input_ids = model_inputs["input_ids"][i]
                
                # ğŸ¯ í•µì‹¬: ë¬¸ìì—´ ê¸°ë°˜ìœ¼ë¡œ assistant ë¶€ë¶„ ì°¾ê¸°
                text_str = text
                
                # assistant í—¤ë” ë¶€ë¶„ê³¼ ì‘ë‹µ ë¶€ë¶„ ë¶„ë¦¬
                assistant_header = "<|start_header_id|>assistant<|end_header_id|>\n\n"
                
                if assistant_header in text_str:
                    # assistant ì‘ë‹µ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
                    assistant_start_pos = text_str.find(assistant_header) + len(assistant_header)
                    
                    # ë§ˆì§€ë§‰ <|eot_id|> ì°¾ê¸°
                    last_eot_pos = text_str.rfind("<|eot_id|>")
                    
                    if last_eot_pos > assistant_start_pos:
                        # assistant ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                        assistant_response = text_str[assistant_start_pos:last_eot_pos]
                        
                        # ğŸ”¥ í•µì‹¬: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ assistant ì‘ë‹µ ë¶€ë¶„ì˜ í† í° ìœ„ì¹˜ ì°¾ê¸°
                        before_assistant = text_str[:assistant_start_pos]
                        
                        # before_assistantë¥¼ í† í¬ë‚˜ì´ì§•í•´ì„œ ê¸¸ì´ í™•ì¸
                        before_tokens = tokenizer(before_assistant, add_special_tokens=False)["input_ids"]
                        assistant_tokens = tokenizer(assistant_response, add_special_tokens=False)["input_ids"]
                        
                        # labels ì´ˆê¸°í™”
                        label = [-100] * len(input_ids)
                        
                        # ğŸ¯ ì¤‘ìš”: ì‹¤ì œ í† í° ì¸ë±ìŠ¤ì—ì„œ assistant ë¶€ë¶„ ì°¾ê¸°
                        if len(assistant_tokens) > 0:
                            # assistant í† í°ë“¤ì´ ì „ì²´ input_idsì—ì„œ ì‹œì‘í•˜ëŠ” ìœ„ì¹˜ ì°¾ê¸°
                            for start_idx in range(len(input_ids) - len(assistant_tokens) + 1):
                                # assistant_tokensì™€ ì¼ì¹˜í•˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
                                if input_ids[start_idx:start_idx + len(assistant_tokens)] == assistant_tokens:
                                    # ì°¾ì•˜ìœ¼ë©´ í•´ë‹¹ ë¶€ë¶„ë§Œ í•™ìŠµ ëŒ€ìƒìœ¼ë¡œ ì„¤ì •
                                    for j in range(start_idx, start_idx + len(assistant_tokens)):
                                        if j < len(input_ids):
                                            label[j] = input_ids[j]
                                    break
                            else:
                                # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ: ë’¤ì—ì„œë¶€í„° ì°¾ê¸°
                                estimated_start = len(input_ids) - len(assistant_tokens) - 2
                                if estimated_start > 0:
                                    for j in range(estimated_start, len(input_ids) - 1):
                                        if j >= 0:
                                            label[j] = input_ids[j]
                    else:
                        print(f"Warning: assistant ì‘ë‹µ ë¶€ë¶„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                        label = [-100] * len(input_ids)
                else:
                    print(f"Warning: assistant í—¤ë”ê°€ ì—†ìŠµë‹ˆë‹¤")
                    label = [-100] * len(input_ids)
                
                labels.append(label)
            
            model_inputs["labels"] = labels
            return model_inputs
        
        # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
        tokenized_dataset = dataset.map(
            tokenize_function, 
            batched=True,
            remove_columns=dataset.column_names
        )
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)
        
        print(f"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"- í›ˆë ¨ ìƒ˜í”Œ: {len(split_dataset['train'])}ê°œ")
        print(f"- ê²€ì¦ ìƒ˜í”Œ: {len(split_dataset['test'])}ê°œ")
        
        return split_dataset['train'], split_dataset['test']
    
    def _setup_training_arguments(self, output_dir: str):
        """í›ˆë ¨ ì„¤ì •"""
        training_args = TrainingArguments(
            output_dir=output_dir,
            overwrite_output_dir=True,
            
            # ì„¤ì • ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
            num_train_epochs=self.epochs,
            per_device_train_batch_size=1,
            per_device_eval_batch_size=1,
            gradient_accumulation_steps=8,
            
            # í•™ìŠµë¥ 
            learning_rate=self.learning_rate,
            weight_decay=0.01,
            lr_scheduler_type="linear",
            warmup_steps=5,
            
            # ì €ì¥/í‰ê°€
            save_strategy="epoch",
            save_total_limit=1,
            eval_strategy="epoch",
            
            # ë¡œê¹…
            logging_dir=f"{output_dir}/logs",
            logging_steps=10,
            
            # ê¸°íƒ€
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            report_to=[],
            
            # ì •ë°€ë„
            fp16=True,
            bf16=False,
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            gradient_checkpointing=False,
            dataloader_num_workers=0,
            max_grad_norm=0.5,
            
            load_best_model_at_end=False,
        )
        
        print("ğŸ›ï¸ í›ˆë ¨ ì„¤ì •:")
        print(f"- ì—í­: {training_args.num_train_epochs}")
        print(f"- í•™ìŠµë¥ : {training_args.learning_rate}")
        print(f"- ì¶œë ¥ ê²½ë¡œ: {output_dir}")
        
        return training_args
    
    def run_automated_finetuning(self, dataset_path: str) -> Dict[str, Any]:
        """ìë™í™”ëœ íŒŒì¸íŠœë‹ ì‹¤í–‰"""
        start_time = datetime.now()
        output_dir = self.base_output_dir / self.current_version
        
        try:
            print("ğŸš€ ìë™í™”ëœ íŒŒì¸íŠœë‹ ì‹œì‘!")
            print("=" * 70)
            
            # 1. ê¸°ì¡´ ëª¨ë¸ ë°±ì—… ê´€ë¦¬
            print("ğŸ’¾ ê¸°ì¡´ ëª¨ë¸ ë°±ì—… ê´€ë¦¬...")
            self._backup_existing_models()
            
            # 2. ë°ì´í„° ë³€í™˜
            print("ğŸ”„ í•™ìŠµ ë°ì´í„° ì¤€ë¹„...")
            converted_dataset_path = self._convert_conversations_to_training_format(dataset_path)
            
            # 3. ê¸°ì¡´ ìµœì‹  ì–´ëŒ‘í„° ì°¾ê¸°
            latest_model_path = self._get_latest_model_path()
            existing_adapter_path = str(latest_model_path) if latest_model_path else None
            
            # 4. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •
            model, tokenizer = self._setup_model_and_tokenizer(existing_adapter_path)
            
            # 5. LoRA ì„¤ì • (ê¸°ì¡´ ì–´ëŒ‘í„°ê°€ ì—†ì„ ë•Œë§Œ)
            if not isinstance(model, PeftModel):
                lora_config = self._setup_lora_config()
                model = get_peft_model(model, lora_config)
            
            model.train()
            
            # í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸
            if hasattr(model, 'print_trainable_parameters'):
                model.print_trainable_parameters()
            
            # 6. ë°ì´í„°ì…‹ ì¤€ë¹„
            train_dataset, eval_dataset = self._load_and_prepare_dataset(tokenizer, converted_dataset_path)
            
            # 7. ë°ì´í„° ì½œë ˆì´í„°
            data_collator = CustomDataCollator(
                tokenizer=tokenizer,
                pad_to_multiple_of=8,
            )
            
            # 8. í›ˆë ¨ ì„¤ì •
            training_args = self._setup_training_arguments(str(output_dir))
            
            # 9. íŠ¸ë ˆì´ë„ˆ ì„¤ì •
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                data_collator=data_collator,
                tokenizer=tokenizer,
            )
            
            # 10. í›ˆë ¨ ì‹œì‘!
            print(f"\nğŸ¯ í›ˆë ¨ ì‹œì‘ (ë²„ì „: {self.current_version})...")
            training_output = trainer.train()
            
            # 11. ëª¨ë¸ ì €ì¥
            print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
            trainer.save_model()
            tokenizer.save_pretrained(str(output_dir))
            
            # 12. í›ˆë ¨ ë¡œê·¸ ê¸°ë¡
            end_time = datetime.now()
            training_time = (end_time - start_time).total_seconds()
            
            log_entry = {
                "version": self.current_version,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "training_time_seconds": training_time,
                "dataset_path": dataset_path,
                "training_samples": len(train_dataset),
                "hyperparameters": {
                    "epochs": self.epochs,
                    "learning_rate": self.learning_rate,
                    "lora_r": self.lora_r,
                    "lora_alpha": self.lora_alpha,
                },
                "output_path": str(output_dir),
                "success": True
            }
            
            self.training_log.append(log_entry)
            self._save_training_log()
            
            print(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! (ë²„ì „: {self.current_version})")
            print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {output_dir}")
            print(f"â±ï¸ ì†Œìš” ì‹œê°„: {training_time:.2f}ì´ˆ")
            
            # ë‹¤ìŒ ë²„ì „ ë²ˆí˜¸ ì—…ë°ì´íŠ¸
            self.current_version = self._get_next_version()
            
            return {
                "success": True,
                "version": log_entry["version"],
                "output_path": str(output_dir),
                "training_time": training_time,
                "training_samples": len(train_dataset),
                "log_entry": log_entry
            }
            
        except Exception as e:
            # ì‹¤íŒ¨ ë¡œê·¸ ê¸°ë¡
            end_time = datetime.now()
            error_log = {
                "version": self.current_version,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "error": str(e),
                "success": False
            }
            
            self.training_log.append(error_log)
            self._save_training_log()
            
            print(f"âŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: {e}")
            raise e
    
    def _save_training_log(self):
        """í›ˆë ¨ ë¡œê·¸ ì €ì¥"""
        log_path = self.base_output_dir / "training_log.json"
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(self.training_log, f, ensure_ascii=False, indent=2)
    
    def get_training_history(self) -> List[Dict[str, Any]]:
        """í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.training_log.copy()
    
    def get_model_versions(self) -> List[Dict[str, Any]]:
        """ëª¨ë¸ ë²„ì „ ëª©ë¡ ë°˜í™˜"""
        versions = []
        
        if not self.base_output_dir.exists():
            return versions
        
        for item in self.base_output_dir.iterdir():
            if item.is_dir() and item.name.startswith(self.version_prefix):
                try:
                    version_num = int(item.name[len(self.version_prefix):])
                    
                    # ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘
                    version_info = {
                        "version": item.name,
                        "version_number": version_num,
                        "path": str(item),
                        "created_time": datetime.fromtimestamp(item.stat().st_ctime).isoformat(),
                        "size_mb": sum(f.stat().st_size for f in item.rglob('*') if f.is_file()) / (1024*1024)
                    }
                    
                    versions.append(version_info)
                    
                except ValueError:
                    continue
        
        # ë²„ì „ ë²ˆí˜¸ë¡œ ì •ë ¬
        versions.sort(key=lambda x: x["version_number"], reverse=True)
        return versions