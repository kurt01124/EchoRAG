import json
import os
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path
import portalocker as fcntl
import threading
from dataclasses import dataclass, asdict

@dataclass
class ConversationEntry:
    """ëŒ€í™” ì—”íŠ¸ë¦¬ ë°ì´í„° í´ë˜ìŠ¤"""
    user_message: str
    assistant_response: str
    timestamp: str
    user_id: str = "default"
    session_id: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    
    def to_training_format(self) -> str:
        """íŒŒì¸íŠœë‹ìš© í˜•íƒœë¡œ ë³€í™˜"""
        return f"USER : {self.user_message}<\\n>ASSISTANT : {self.assistant_response}"
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)

class ConversationCollector:
    """ì‹¤ì‹œê°„ ëŒ€í™” ìˆ˜ì§‘ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.enabled = config.get("enabled", True)
        self.min_length = config.get("min_length", 5)
        self.max_length = config.get("max_length", 2000)
        self.filter_system = config.get("filter_system", True)
        self.data_path = config.get("data_path", "./data/finetune")
        self.file_name = config.get("file_name", "conversations.jsonl")
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.file_path = Path(self.data_path) / self.file_name
        self.backup_path = Path(self.data_path) / f"backup_{self.file_name}"
        
        # í†µê³„
        self.stats = {
            "total_collected": 0,
            "filtered_out": 0,
            "last_collection": None,
            "file_size_kb": 0
        }
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½
        self.lock = threading.Lock()
        
        # ì´ˆê¸°í™”
        self._ensure_directory()
        self._load_stats()
        
        print(f"ğŸ“š ëŒ€í™” ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”: {'í™œì„±í™”' if self.enabled else 'ë¹„í™œì„±í™”'}")
        if self.enabled:
            print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {self.file_path}")
            print(f"ğŸ“ ìˆ˜ì§‘ ì¡°ê±´: {self.min_length}~{self.max_length}ì")
            print(f"ğŸ“Š í˜„ì¬ í†µê³„: {self.stats['total_collected']}ê°œ ìˆ˜ì§‘ë¨")
    
    def _ensure_directory(self):
        """ë””ë ‰í„°ë¦¬ ìƒì„±"""
        self.file_path.parent.mkdir(parents=True, exist_ok=True)
    
    def _load_stats(self):
        """í†µê³„ ë¡œë“œ"""
        if self.file_path.exists():
            try:
                # íŒŒì¼ í¬ê¸° ê³„ì‚°
                self.stats["file_size_kb"] = round(self.file_path.stat().st_size / 1024, 2)
                
                # ë¼ì¸ ìˆ˜ ê³„ì‚° (ì´ ìˆ˜ì§‘ ê°œìˆ˜)
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    lines = sum(1 for _ in f)
                    self.stats["total_collected"] = lines
                    
            except Exception as e:
                print(f"âš ï¸ í†µê³„ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def is_valid_conversation(self, user_message: str, assistant_response: str) -> tuple[bool, str]:
        """ëŒ€í™” ìœ íš¨ì„± ê²€ì¦"""
        if not self.enabled:
            return False, "ìˆ˜ì§‘ ë¹„í™œì„±í™”ë¨"
        
        # ê¸¸ì´ ê²€ì¦
        user_len = len(user_message.strip())
        assistant_len = len(assistant_response.strip())
        
        if user_len < self.min_length:
            return False, f"ì‚¬ìš©ì ë©”ì‹œì§€ ë„ˆë¬´ ì§§ìŒ ({user_len}ì < {self.min_length}ì)"
        
        if assistant_len < self.min_length:
            return False, f"ì‘ë‹µ ë©”ì‹œì§€ ë„ˆë¬´ ì§§ìŒ ({assistant_len}ì < {self.min_length}ì)"
        
        if user_len > self.max_length:
            return False, f"ì‚¬ìš©ì ë©”ì‹œì§€ ë„ˆë¬´ ê¹€ ({user_len}ì > {self.max_length}ì)"
        
        if assistant_len > self.max_length:
            return False, f"ì‘ë‹µ ë©”ì‹œì§€ ë„ˆë¬´ ê¹€ ({assistant_len}ì > {self.max_length}ì)"
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ í•„í„°ë§
        if self.filter_system:
            system_keywords = [
                "ì´ˆê¸°í™”", "ì„¤ì •", "ì˜¤ë¥˜", "ì„œë²„", "ëª¨ë¸", "ë¡œë”©", "API", 
                "ì‹œìŠ¤í…œ", "ì—ëŸ¬", "debug", "test", "health", "status"
            ]
            
            combined_text = (user_message + " " + assistant_response).lower()
            for keyword in system_keywords:
                if keyword in combined_text:
                    return False, f"ì‹œìŠ¤í…œ ë©”ì‹œì§€ í•„í„°ë§ë¨ (í‚¤ì›Œë“œ: {keyword})"
        
        return True, "ìœ íš¨í•¨"
    
    def collect_conversation(
        self, 
        user_message: str, 
        assistant_response: str,
        user_id: str = "default",
        session_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """ëŒ€í™” ìˆ˜ì§‘"""
        
        # ìœ íš¨ì„± ê²€ì¦
        is_valid, reason = self.is_valid_conversation(user_message, assistant_response)
        
        if not is_valid:
            self.stats["filtered_out"] += 1
            print(f"ğŸš« ëŒ€í™” í•„í„°ë§: {reason}")
            return False
        
        # ëŒ€í™” ì—”íŠ¸ë¦¬ ìƒì„±
        entry = ConversationEntry(
            user_message=user_message.strip(),
            assistant_response=assistant_response.strip(),
            timestamp=datetime.now().isoformat(),
            user_id=user_id,
            session_id=session_id,
            metadata=metadata or {}
        )
        
        # íŒŒì¼ì— ì €ì¥ (ìŠ¤ë ˆë“œ ì•ˆì „)
        success = self._save_to_file(entry)
        
        if success:
            self.stats["total_collected"] += 1
            self.stats["last_collection"] = entry.timestamp
            print(f"ğŸ’¾ ëŒ€í™” ìˆ˜ì§‘ ì™„ë£Œ: {self.stats['total_collected']}ë²ˆì§¸")
            return True
        else:
            print(f"âŒ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨")
            return False
    
    def _save_to_file(self, entry: ConversationEntry) -> bool:
        """íŒŒì¼ì— ì•ˆì „í•˜ê²Œ ì €ì¥"""
        try:
            with self.lock:
                # JSONL í˜•íƒœë¡œ ì €ì¥
                with open(self.file_path, 'a', encoding='utf-8') as f:
                    # íŒŒì¼ ë½í‚¹ (ë‹¤ì¤‘ í”„ë¡œì„¸ìŠ¤ í™˜ê²½ì—ì„œ ì•ˆì „)
                    fcntl.flock(f.fileno(), fcntl.LOCK_EX)
                    try:
                        json.dump(entry.to_dict(), f, ensure_ascii=False)
                        f.write('\n')
                    finally:
                        fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                
                # íŒŒì¼ í¬ê¸° ì—…ë°ì´íŠ¸
                self.stats["file_size_kb"] = round(self.file_path.stat().st_size / 1024, 2)
            
            return True
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def get_collected_conversations(self, limit: Optional[int] = None) -> List[ConversationEntry]:
        """ìˆ˜ì§‘ëœ ëŒ€í™” ëª©ë¡ ë°˜í™˜"""
        conversations = []
        
        if not self.file_path.exists():
            return conversations
        
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
                # ìµœì‹  ìˆœìœ¼ë¡œ ì œí•œ
                if limit:
                    lines = lines[-limit:]
                
                for line in lines:
                    if line.strip():
                        data = json.loads(line.strip())
                        conversations.append(ConversationEntry(**data))
            
            return conversations
            
        except Exception as e:
            print(f"âŒ ëŒ€í™” ë¡œë“œ ì˜¤ë¥˜: {e}")
            return []
    
    def get_training_data(self, limit: Optional[int] = None) -> List[str]:
        """íŒŒì¸íŠœë‹ìš© í˜•íƒœë¡œ ëŒ€í™” ë°˜í™˜"""
        conversations = self.get_collected_conversations(limit)
        return [conv.to_training_format() for conv in conversations]
    
    def get_stats(self) -> Dict[str, Any]:
        """ìˆ˜ì§‘ í†µê³„ ë°˜í™˜"""
        # ì‹¤ì‹œê°„ íŒŒì¼ í¬ê¸° ì—…ë°ì´íŠ¸
        if self.file_path.exists():
            self.stats["file_size_kb"] = round(self.file_path.stat().st_size / 1024, 2)
        
        return {
            **self.stats,
            "enabled": self.enabled,
            "file_path": str(self.file_path),
            "collection_criteria": {
                "min_length": self.min_length,
                "max_length": self.max_length,
                "filter_system": self.filter_system
            }
        }
    
    def backup_conversations(self) -> bool:
        """ëŒ€í™” ë°ì´í„° ë°±ì—…"""
        if not self.file_path.exists():
            print("âš ï¸ ë°±ì—…í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ë°±ì—… íŒŒì¼ëª…
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"conversations_backup_{timestamp}.jsonl"
            backup_full_path = Path(self.data_path) / backup_name
            
            # íŒŒì¼ ë³µì‚¬
            import shutil
            shutil.copy2(self.file_path, backup_full_path)
            
            print(f"ğŸ’¾ ëŒ€í™” ë°±ì—… ì™„ë£Œ: {backup_full_path}")
            return True
            
        except Exception as e:
            print(f"âŒ ë°±ì—… ì‹¤íŒ¨: {e}")
            return False
    
    def clear_conversations(self, backup_first: bool = True) -> bool:
        """ëŒ€í™” ë°ì´í„° ì´ˆê¸°í™”"""
        if backup_first:
            self.backup_conversations()
        
        try:
            if self.file_path.exists():
                self.file_path.unlink()
            
            # í†µê³„ ì´ˆê¸°í™”
            self.stats = {
                "total_collected": 0,
                "filtered_out": 0,
                "last_collection": None,
                "file_size_kb": 0
            }
            
            print("ğŸ—‘ï¸ ëŒ€í™” ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def export_for_finetuning(self, output_path: Optional[str] = None) -> Optional[str]:
        """íŒŒì¸íŠœë‹ìš© ë°ì´í„°ì…‹ ìƒì„±"""
        if output_path is None:
            output_path = Path(self.data_path) / "training_dataset.json"
        else:
            output_path = Path(output_path)
        
        conversations = self.get_collected_conversations()
        
        if not conversations:
            print("âš ï¸ ë‚´ë³´ë‚¼ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            # íŒŒì¸íŠœë‹ìš© ë°ì´í„° í˜•íƒœë¡œ ë³€í™˜
            training_data = []
            
            for conv in conversations:
                # TARGET íƒœê·¸ë¥¼ ì‚¬ìš©í•œ ë²¡í„° ë³µì› í•™ìŠµ í˜•íƒœ
                conversation_text = conv.to_training_format()
                
                training_sample = {
                    "input": f"<TARGET>{conversation_text}</TARGET>TARGET íƒœê·¸ ì•ˆì˜ ë‚´ìš©ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì¶”ê°€ ì„¤ëª…ì€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.",
                    "output": conversation_text,
                    "metadata": {
                        "timestamp": conv.timestamp,
                        "user_id": conv.user_id,
                        "session_id": conv.session_id,
                        "source": "conversation_collector",
                        "text_length": len(conversation_text)
                    }
                }
                
                training_data.append(training_sample)
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(training_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“Š íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(training_data)}ê°œ ìƒ˜í”Œ")
            print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {output_path}")
            
            return str(output_path)
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def should_trigger_training(self, batch_size: int) -> bool:
        """íŒŒì¸íŠœë‹ íŠ¸ë¦¬ê±° ì¡°ê±´ í™•ì¸"""
        return self.stats["total_collected"] >= batch_size and self.stats["total_collected"] % batch_size == 0
    
    def get_pending_training_count(self, batch_size: int) -> int:
        """ë‹¤ìŒ í•™ìŠµê¹Œì§€ ë‚¨ì€ ëŒ€í™” ìˆ˜"""
        current = self.stats["total_collected"]
        return batch_size - (current % batch_size) if current % batch_size != 0 else 0


class MLOpsDataManager:
    """MLOpsë¥¼ ìœ„í•œ í†µí•© ë°ì´í„° ê´€ë¦¬ì"""
    
    def __init__(self, finetune_config: Dict[str, Any], conversation_config: Dict[str, Any]):
        self.finetune_config = finetune_config
        self.conversation_config = conversation_config
        
        # ëŒ€í™” ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        self.collector = ConversationCollector(conversation_config)
        
        # íŒŒì¸íŠœë‹ ê´€ë ¨ ì„¤ì •
        self.batch_size = finetune_config.get("batch_size", 50)
        self.auto_trigger = finetune_config.get("auto_trigger", True)
        self.models_path = Path(finetune_config.get("models_path", "./models"))
        self.backup_count = finetune_config.get("backup_count", 3)
        
        # ìƒíƒœ ì¶”ì 
        self.last_training_count = 0
        self.training_in_progress = False
        
        print(f"ğŸ¤– MLOps ë°ì´í„° ë§¤ë‹ˆì € ì´ˆê¸°í™”")
        print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {self.batch_size}ê°œ ëŒ€í™”")
        print(f"âš¡ ìë™ íŠ¸ë¦¬ê±°: {'ON' if self.auto_trigger else 'OFF'}")
    
    def process_conversation(
        self, 
        user_message: str, 
        assistant_response: str,
        user_id: str = "default",
        session_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """ëŒ€í™” ì²˜ë¦¬ ë° íŒŒì¸íŠœë‹ íŠ¸ë¦¬ê±° í™•ì¸"""
        
        # ëŒ€í™” ìˆ˜ì§‘
        collected = self.collector.collect_conversation(
            user_message=user_message,
            assistant_response=assistant_response,
            user_id=user_id,
            session_id=session_id,
            metadata=metadata
        )
        
        result = {
            "collected": collected,
            "total_collected": self.collector.stats["total_collected"],
            "should_train": False,
            "pending_count": 0,
            "training_triggered": False
        }
        
        if collected and self.auto_trigger:
            # íŒŒì¸íŠœë‹ íŠ¸ë¦¬ê±° í™•ì¸
            should_train = self.collector.should_trigger_training(self.batch_size)
            result["should_train"] = should_train
            result["pending_count"] = self.collector.get_pending_training_count(self.batch_size)
            
            if should_train and not self.training_in_progress:
                # ìë™ íŒŒì¸íŠœë‹ íŠ¸ë¦¬ê±° (ë¹„ë™ê¸° ì‹¤í–‰)
                result["training_triggered"] = self._trigger_async_training()
        
        return result
    
    def _trigger_async_training(self) -> bool:
        """ë¹„ë™ê¸° íŒŒì¸íŠœë‹ íŠ¸ë¦¬ê±°"""
        try:
            import threading
            
            def training_worker():
                self.training_in_progress = True
                try:
                    print("ğŸš€ ìë™ íŒŒì¸íŠœë‹ ì‹œì‘...")
                    success = self.start_finetuning()
                    if success:
                        print("âœ… ìë™ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
                    else:
                        print("âŒ ìë™ íŒŒì¸íŠœë‹ ì‹¤íŒ¨!")
                finally:
                    self.training_in_progress = False
            
            # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            training_thread = threading.Thread(target=training_worker, daemon=True)
            training_thread.start()
            
            return True
            
        except Exception as e:
            print(f"âŒ íŒŒì¸íŠœë‹ íŠ¸ë¦¬ê±° ì‹¤íŒ¨: {e}")
            self.training_in_progress = False
            return False
    
    def start_finetuning(self) -> bool:
        """íŒŒì¸íŠœë‹ ì‹œì‘"""
        try:
            # 1. ë°ì´í„°ì…‹ ìƒì„±
            dataset_path = self.collector.export_for_finetuning()
            if not dataset_path:
                print("âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")
                return False
            
            # 2. ëª¨ë¸ ë°±ì—… (ê¸°ì¡´ ì–´ëŒ‘í„°ê°€ ìˆë‹¤ë©´)
            self._backup_existing_model()
            
            # 3. íŒŒì¸íŠœë‹ ì‹¤í–‰ (ì—¬ê¸°ì„œëŠ” ìŠ¤í¬ë¦½íŠ¸ í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜)
            print("ğŸ§  íŒŒì¸íŠœë‹ ì‹¤í–‰ ì¤‘...")
            success = self._run_finetuning_script(dataset_path)
            
            if success:
                # 4. ëª¨ë¸ ë²„ì „ ì—…ë°ì´íŠ¸
                self._update_model_version()
                self.last_training_count = self.collector.stats["total_collected"]
                return True
            else:
                print("âŒ íŒŒì¸íŠœë‹ ì‹¤í–‰ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ íŒŒì¸íŠœë‹ ì˜¤ë¥˜: {e}")
            return False
    
    def _backup_existing_model(self):
        """ê¸°ì¡´ ëª¨ë¸ ë°±ì—…"""
        # êµ¬í˜„ ì˜ˆì •: ê¸°ì¡´ ì–´ëŒ‘í„° ë°±ì—… ë¡œì§
        print("ğŸ’¾ ê¸°ì¡´ ëª¨ë¸ ë°±ì—… ì¤‘...")
    
    def _run_finetuning_script(self, dataset_path: str) -> bool:
        """íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰"""
        # ì‹¤ì œë¡œëŠ” finetuning.pyë¥¼ subprocessë¡œ ì‹¤í–‰
        print(f"ğŸš€ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰: {dataset_path}")
        # ì‹œë®¬ë ˆì´ì…˜
        import time
        time.sleep(2)  # ì‹¤ì œë¡œëŠ” ë” ì˜¤ë˜ ê±¸ë¦¼
        return True
    
    def _update_model_version(self):
        """ëª¨ë¸ ë²„ì „ ì—…ë°ì´íŠ¸"""
        # êµ¬í˜„ ì˜ˆì •: ë²„ì „ ê´€ë¦¬ ë¡œì§
        print("ğŸ”„ ëª¨ë¸ ë²„ì „ ì—…ë°ì´íŠ¸ ì¤‘...")
    
    def get_status(self) -> Dict[str, Any]:
        """ì „ì²´ ìƒíƒœ ë°˜í™˜"""
        collector_stats = self.collector.get_stats()
        
        return {
            "collector": collector_stats,
            "training": {
                "batch_size": self.batch_size,
                "auto_trigger": self.auto_trigger,
                "in_progress": self.training_in_progress,
                "last_training_count": self.last_training_count,
                "pending_count": self.collector.get_pending_training_count(self.batch_size),
                "should_train": self.collector.should_trigger_training(self.batch_size)
            },
            "models": {
                "path": str(self.models_path),
                "backup_count": self.backup_count
            }
        }